{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97f01692",
   "metadata": {},
   "source": [
    "# Preprocess Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8deec1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab8ecc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "from email.parser import Parser\n",
    "from email import policy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5721d0f",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7c19f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Phishing dataset already exists. Skipping download.\n",
      "âœ… Enron dataset already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Directory structure\n",
    "RAW_DATA_DIR = Path(\"data/raw\")\n",
    "DATA_PREPROCESSED_DIR = Path(\"data/preprocessed\")\n",
    "\n",
    "# Create directories\n",
    "RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATA_PREPROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dataset names on Kaggle\n",
    "PHISHING_DATASET = \"subhajournal/phishingemails\"\n",
    "ENRON_DATASET = \"wcukierski/enron-email-dataset\"\n",
    "\n",
    "def download_datasets():\n",
    "    \"\"\"Download datasets from Kaggle if not already present.\"\"\"\n",
    "    \n",
    "    # Download phishing dataset\n",
    "    phishing_dir = RAW_DATA_DIR / \"phishing\"\n",
    "    phishing_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    if not any(phishing_dir.iterdir()):\n",
    "        print(f\"Downloading phishing dataset: {PHISHING_DATASET}...\")\n",
    "        os.system(f\"kaggle datasets download -d {PHISHING_DATASET} -p {phishing_dir} --unzip\")\n",
    "        print(\"âœ… Phishing dataset downloaded!\")\n",
    "    else:\n",
    "        print(\"âœ… Phishing dataset already exists. Skipping download.\")\n",
    "    \n",
    "    # Download Enron dataset\n",
    "    enron_dir = RAW_DATA_DIR / \"enron\"\n",
    "    enron_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    if not any(enron_dir.iterdir()):\n",
    "        print(f\"Downloading Enron dataset: {ENRON_DATASET}...\")\n",
    "        os.system(f\"kaggle datasets download -d {ENRON_DATASET} -p {enron_dir} --unzip\")\n",
    "        print(\"âœ… Enron dataset downloaded!\")\n",
    "    else:\n",
    "        print(\"âœ… Enron dataset already exists. Skipping download.\")\n",
    "\n",
    "\n",
    "download_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec66af1a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e9a4ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHISHING DATASET:\n",
      "Dataset shape: (18650, 3)\n",
      "Columns: ['Unnamed: 0', 'Email Text', 'Email Type']\n",
      "First few rows:\n",
      "   Unnamed: 0                                         Email Text  Email Type\n",
      "0           0  re : 6 . 1100 , disc : uniformitarianism , re ...  Safe Email\n",
      "1           1  the other side of * galicismos * * galicismo *...  Safe Email\n",
      "2           2  re : equistar deal tickets are you still avail...  Safe Email\n",
      "Label distribution:\n",
      "Email Type\n",
      "Safe Email        11322\n",
      "Phishing Email     7328\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ENRON DATASET:\n",
      "Full dataset shape: (517401, 2)\n",
      "Columns: ['file', 'message']\n",
      "Sampling 5000 emails from 517401 total emails...\n",
      "Sampled dataset shape: (5000, 2)\n",
      "First few rows:\n",
      "                         file  \\\n",
      "0     shackleton-s/sent/1912.   \n",
      "1    farmer-d/logistics/1066.   \n",
      "2  parks-j/deleted_items/202.   \n",
      "\n",
      "                                             message  \n",
      "0  Message-ID: <21013688.1075844564560.JavaMail.e...  \n",
      "1  Message-ID: <22688499.1075854130303.JavaMail.e...  \n",
      "2  Message-ID: <27817771.1075841359502.JavaMail.e...  \n"
     ]
    }
   ],
   "source": [
    "def load_phishing_dataset():\n",
    "    \"\"\"Load the phishing email dataset.\"\"\"\n",
    "    \n",
    "    phishing_dir = RAW_DATA_DIR / \"phishing\"\n",
    "    \n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(phishing_dir / \"Phishing_Email.csv\")\n",
    "\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(f\"First few rows:\")\n",
    "    print(df.head(3))\n",
    "    \n",
    "    # Check for label distribution\n",
    "    print(f\"Label distribution:\")\n",
    "    print(df['Email Type'].value_counts())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_enron_dataset(sample_size=5000):\n",
    "    \"\"\"Load a sample of the Enron email dataset.\n",
    "    \n",
    "    Args:\n",
    "        sample_size: Number of emails to sample (Enron is very large)\n",
    "    \"\"\"\n",
    "    enron_dir = RAW_DATA_DIR / \"enron\"\n",
    "    \n",
    "    df = pd.read_csv(enron_dir / \"emails.csv\")\n",
    "  \n",
    "    print(f\"Full dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Sample the dataset\n",
    "    if len(df) > sample_size:\n",
    "        print(f\"Sampling {sample_size} emails from {len(df)} total emails...\")\n",
    "        df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Sampled dataset shape: {df.shape}\")\n",
    "    print(f\"First few rows:\")\n",
    "    print(df.head(3))\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"PHISHING DATASET:\")\n",
    "phishing_df = load_phishing_dataset()\n",
    "\n",
    "print(\"\\nENRON DATASET:\")\n",
    "enron_df = load_enron_dataset(sample_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb70a21",
   "metadata": {},
   "source": [
    "## Preprocess Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d46f3421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phishing Dataset Preprocessing:\n",
      "Removed 38 short emails (length < 2)\n",
      "Processed 18612 emails\n",
      "Label distribution:\n",
      "label\n",
      "0    11309\n",
      "1     7303\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Enron Dataset Preprocessing:\n",
      "Cleaning text...\n",
      "Removed 3 short emails (length < 2)\n",
      "Processed 4997 legitimate emails\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean email text.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def parse_email_body_to_str(text):\n",
    "    \"\"\"Extract email body from raw email text if needed.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    # Try to parse as email if it looks like raw email format\n",
    "    if \"Subject:\" in text or \"From:\" in text:\n",
    "        try:\n",
    "            parser = Parser(policy=policy.default)\n",
    "            msg = parser.parsestr(text)\n",
    "            \n",
    "            # Get email body\n",
    "            if msg.is_multipart():\n",
    "                body = \"\"\n",
    "                for part in msg.walk():\n",
    "                    if part.get_content_type() == \"text/plain\":\n",
    "                        body += part.get_payload(decode=True).decode('utf-8', errors='ignore')\n",
    "            else:\n",
    "                body = msg.get_payload(decode=True).decode('utf-8', errors='ignore')\n",
    "            \n",
    "            return body\n",
    "        except:\n",
    "            return text\n",
    "        \n",
    "    return text\n",
    "    \n",
    "def remove_short_emails(df, min_length=5):\n",
    "    \"\"\"Remove emails with text shorter than min_length.\"\"\"\n",
    "    initial_count = len(df)\n",
    "    df = df[df['text'].str.len() >= min_length].reset_index(drop=True)\n",
    "    final_count = len(df)\n",
    "    print(f\"Removed {initial_count - final_count} short emails (length < {min_length})\")\n",
    "    return df\n",
    "\n",
    "def preprocess_phishing_dataset(df):\n",
    "    \"\"\"Preprocess the phishing dataset to standard format.\"\"\"\n",
    "    \n",
    "    # Create a standardized dataframe\n",
    "    processed_df = pd.DataFrame()\n",
    "    \n",
    "    # Try to identify text and label columns\n",
    "    processed_df['text'] = df['Email Text']\n",
    "    \n",
    "    # Extract label\n",
    "    # Map Email Type to binary label (0: legitimate, 1: phishing)\n",
    "    processed_df['label'] = df['Email Type'].map({\n",
    "        'Safe Email': 0,\n",
    "        'Phishing Email': 1\n",
    "    })\n",
    "   \n",
    "    \n",
    "    # Clean text\n",
    "    processed_df['text'] = processed_df['text'].apply(parse_email_body_to_str)\n",
    "    processed_df['text'] = processed_df['text'].apply(clean_text)\n",
    "    processed_df = remove_short_emails(processed_df, min_length=2)\n",
    "\n",
    "    \n",
    "    print(f\"Processed {len(processed_df)} emails\")\n",
    "    print(f\"Label distribution:\")\n",
    "    print(processed_df['label'].value_counts())\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "def preprocess_enron_dataset(df):\n",
    "    \"\"\"Preprocess the Enron dataset to standard format.\"\"\"\n",
    "    \n",
    "    # Create a standardized dataframe\n",
    "    processed_df = pd.DataFrame()\n",
    "    \n",
    "    # Try to identify text column\n",
    "    processed_df['text'] = df['message']\n",
    "   \n",
    "    \n",
    "    # All Enron emails are legitimate (label = 0)\n",
    "    processed_df['label'] = 0\n",
    "    \n",
    "    # Clean text\n",
    "    print(\"Cleaning text...\")\n",
    "    processed_df['text'] = processed_df['text'].apply(parse_email_body_to_str)\n",
    "    processed_df['text'] = processed_df['text'].apply(clean_text)\n",
    "\n",
    "    processed_df = remove_short_emails(processed_df, min_length=2)\n",
    "\n",
    "\n",
    "    print(f\"Processed {len(processed_df)} legitimate emails\")\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "print(\"Phishing Dataset Preprocessing:\")\n",
    "phishing_processed_df = preprocess_phishing_dataset(phishing_df)\n",
    "\n",
    "print(\"\\nEnron Dataset Preprocessing:\")\n",
    "enron_processed_df = preprocess_enron_dataset(enron_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e9549",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a87d2070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "try:\n",
    "        STOPWORDS = set(stopwords.words('english'))\n",
    "except:\n",
    "    print(\"âš ï¸ Downloading NLTK stopwords...\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Urgent keywords for phishing detection\n",
    "URGENT_KEYWORDS = [\n",
    "    'urgent', 'immediately', 'action required', 'verify', 'confirm', \n",
    "    'suspend', 'restricted', 'expired', 'update', 'click here',\n",
    "    'act now', 'limited time', 'alert', 'warning', 'attention',\n",
    "    'security', 'unusual activity', 'locked', 'expiring', 'renewal'\n",
    "]\n",
    "\n",
    "def extract_num_words(text):\n",
    "    \"\"\"Extract total number of words.\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return 0\n",
    "    words = text.split()\n",
    "    return len(words)\n",
    "\n",
    "def extract_num_unique_words(text):\n",
    "    \"\"\"Extract count of unique words.\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return 0\n",
    "    words = [word.lower() for word in text.split()]\n",
    "    return len(set(words))\n",
    "\n",
    "def extract_num_stopwords(text):\n",
    "    \"\"\"Extract count of stopwords.\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return 0\n",
    "    words = [word.lower() for word in text.split()]\n",
    "    return sum(1 for word in words if word in STOPWORDS)\n",
    "\n",
    "def extract_num_links(text):\n",
    "    \"\"\"Extract number of hyperlinks.\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return 0\n",
    "    # Match URLs (http, https, www)\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+|www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    links = re.findall(url_pattern, text)\n",
    "    return len(links)\n",
    "\n",
    "def extract_num_unique_domains(text):\n",
    "    \"\"\"Extract number of unique domains in links.\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return 0\n",
    "    # Match URLs and extract domains\n",
    "    url_pattern = r'http[s]?://([a-zA-Z0-9.-]+)|www\\.([a-zA-Z0-9.-]+)'\n",
    "    matches = re.findall(url_pattern, text)\n",
    "    domains = [match[0] if match[0] else match[1] for match in matches]\n",
    "    domains = [d.split('/')[0] for d in domains if d]  # Clean up domains\n",
    "    return len(set(domains))\n",
    "\n",
    "def extract_num_email_addresses(text):\n",
    "    \"\"\"Extract count of email addresses.\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return 0\n",
    "    # Match email addresses\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    emails = re.findall(email_pattern, text)\n",
    "    return len(emails)\n",
    "\n",
    "def extract_num_spelling_errors(text):\n",
    "    \"\"\"Extract count of misspelled words.\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return 0\n",
    "    \n",
    "    # Extract words (alphanumeric only)\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "    \n",
    "    # Filter out very short words and check spelling\n",
    "    words = [w for w in words if len(w) > 2]\n",
    "    \n",
    "    # Limit to first 100 words for performance\n",
    "    words = words[:100]\n",
    "    \n",
    "    misspelled = spell.unknown(words)\n",
    "    return len(misspelled)\n",
    "\n",
    "def extract_num_urgent_keywords(text):\n",
    "    \"\"\"Extract count of urgent keywords.\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return 0\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    count = 0\n",
    "    for keyword in URGENT_KEYWORDS:\n",
    "        count += text_lower.count(keyword)\n",
    "    \n",
    "    return count\n",
    "\n",
    "def extract_features(df):\n",
    "    \"\"\"Extract all features from the text column.\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"FEATURE ENGINEERING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nðŸ”§ Extracting features...\")\n",
    "    \n",
    "    # Extract each feature\n",
    "    print(\"   âœ“ Extracting num_words...\")\n",
    "    df['num_words'] = df['text'].apply(extract_num_words)\n",
    "    \n",
    "    print(\"   âœ“ Extracting num_unique_words...\")\n",
    "    df['num_unique_words'] = df['text'].apply(extract_num_unique_words)\n",
    "    \n",
    "    print(\"   âœ“ Extracting num_stopwords...\")\n",
    "    df['num_stopwords'] = df['text'].apply(extract_num_stopwords)\n",
    "    \n",
    "    print(\"   âœ“ Extracting num_links...\")\n",
    "    df['num_links'] = df['text'].apply(extract_num_links)\n",
    "    \n",
    "    print(\"   âœ“ Extracting num_unique_domains...\")\n",
    "    df['num_unique_domains'] = df['text'].apply(extract_num_unique_domains)\n",
    "    \n",
    "    print(\"   âœ“ Extracting num_email_addresses...\")\n",
    "    df['num_email_addresses'] = df['text'].apply(extract_num_email_addresses)\n",
    "    \n",
    "    print(\"   âœ“ Extracting num_spelling_errors (this may take a while)...\")\n",
    "    df['num_spelling_errors'] = df['text'].apply(extract_num_spelling_errors)\n",
    "    \n",
    "    print(\"   âœ“ Extracting num_urgent_keywords...\")\n",
    "    df['num_urgent_keywords'] = df['text'].apply(extract_num_urgent_keywords)\n",
    "    \n",
    "    print(\"\\nâœ… Feature extraction complete!\")\n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf9404",
   "metadata": {},
   "source": [
    "## Combine Datasets and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "961790d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (23609, 2)\n",
      "Final label distribution:\n",
      "label\n",
      "0    16306\n",
      "1     7303\n",
      "Name: count, dtype: int64\n",
      "Class balance:\n",
      "label\n",
      "0    0.690669\n",
      "1    0.309331\n",
      "Name: proportion, dtype: float64\n",
      "======================================================================\n",
      "FEATURE ENGINEERING\n",
      "======================================================================\n",
      "\n",
      "ðŸ”§ Extracting features...\n",
      "   âœ“ Extracting num_words...\n",
      "   âœ“ Extracting num_unique_words...\n",
      "   âœ“ Extracting num_stopwords...\n",
      "   âœ“ Extracting num_links...\n",
      "   âœ“ Extracting num_unique_domains...\n",
      "   âœ“ Extracting num_email_addresses...\n",
      "   âœ“ Extracting num_spelling_errors (this may take a while)...\n",
      "   âœ“ Extracting num_urgent_keywords...\n",
      "\n",
      "âœ… Feature extraction complete!\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Saved combined dataset to: data/preprocessed/emails_combined.csv\n",
      "Final shape: (23609, 10)\n",
      "Columns: ['text', 'label', 'num_words', 'num_unique_words', 'num_stopwords', 'num_links', 'num_unique_domains', 'num_email_addresses', 'num_spelling_errors', 'num_urgent_keywords']\n",
      "\n",
      "\n",
      "SAMPLE DATA\n",
      "Sample legitimate email with features:\n",
      "Text (first 300 chars): Are these students first or second years? If they are first years, we did not have a single one submit a resume for the Summer Associate position. Michele Nezi Marvin Manager Enron Broadband Services (713)853-6848 Kristin Gandy@ENRON 01/03/01 10:17 AM To: Jeffrey A Shankman/HOU/ECT@ECT, William Keen...\n",
      "\n",
      "Features:\n",
      "  num_words: 256\n",
      "  num_unique_words: 200\n",
      "  num_stopwords: 56\n",
      "  num_links: 0\n",
      "  num_unique_domains: 0\n",
      "  num_email_addresses: 0\n",
      "  num_spelling_errors: 16\n",
      "  num_urgent_keywords: 0\n",
      "\n",
      "Sample phishing email with features:\n",
      "Text (first 300 chars): fwd : great news dear applicant , your application was processed and approved . you are eligible for a 2 . 3 % rate and a $ 400 , 000 loan . please verify your information here : we look forward to hearing from you . regards , tad * sosa senior account manager webber financial association...\n",
      "\n",
      "Features:\n",
      "  num_words: 57\n",
      "  num_unique_words: 47\n",
      "  num_stopwords: 15\n",
      "  num_links: 0\n",
      "  num_unique_domains: 0\n",
      "  num_email_addresses: 0\n",
      "  num_spelling_errors: 2\n",
      "  num_urgent_keywords: 1\n"
     ]
    }
   ],
   "source": [
    "def combine_and_save(phishing_df, enron_df):\n",
    "    \"\"\"Combine datasets and save to CSV.\"\"\"\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_df = pd.concat([phishing_df, enron_df], ignore_index=True)\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "    print(f\"Final label distribution:\")\n",
    "    print(combined_df['label'].value_counts())\n",
    "    print(f\"Class balance:\")\n",
    "    print(combined_df['label'].value_counts(normalize=True))\n",
    "\n",
    "    # Extract features\n",
    "    combined_df = extract_features(combined_df)\n",
    "\n",
    "    # Reorder columns: text, label, then all features\n",
    "    feature_cols = [\n",
    "        'num_words', 'num_unique_words', 'num_stopwords', 'num_links',\n",
    "        'num_unique_domains', 'num_email_addresses', 'num_spelling_errors',\n",
    "        'num_urgent_keywords'\n",
    "    ]\n",
    "    \n",
    "    combined_df = combined_df[['text', 'label'] + feature_cols]\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_file = DATA_PREPROCESSED_DIR / \"emails_combined.csv\"\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved combined dataset to: {output_file}\")\n",
    "    print(f\"Final shape: {combined_df.shape}\")\n",
    "    print(f\"Columns: {combined_df.columns.tolist()}\")\n",
    "\n",
    "    # Show sample emails\n",
    "    print(\"\\n\\nSAMPLE DATA\")\n",
    "    print(\"Sample legitimate email with features:\")\n",
    "    legit_sample = combined_df[combined_df['label'] == 0].iloc[0]\n",
    "    print(f\"Text (first 300 chars): {legit_sample['text'][:300]}...\")\n",
    "    print(f\"\\nFeatures:\")\n",
    "    for col in feature_cols:\n",
    "        print(f\"  {col}: {legit_sample[col]}\")\n",
    "\n",
    "    print(f\"\\nSample phishing email with features:\")\n",
    "    phish_sample = combined_df[combined_df['label'] == 1].iloc[0]\n",
    "    print(f\"Text (first 300 chars): {phish_sample['text'][:300]}...\")\n",
    "    print(f\"\\nFeatures:\")\n",
    "    for col in feature_cols:\n",
    "        print(f\"  {col}: {phish_sample[col]}\")\n",
    "    \n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "\n",
    "# Combine and save\n",
    "combined_df = combine_and_save(phishing_processed_df, enron_processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89d74142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Feature statistics by class:\n",
      "\n",
      "======================================================================\n",
      "LEGITIMATE EMAILS (label=0)\n",
      "======================================================================\n",
      "          num_words  num_unique_words  num_stopwords     num_links  \\\n",
      "count  16306.000000      16306.000000   16306.000000  16306.000000   \n",
      "mean     342.292714        151.941555     101.779406      0.883908   \n",
      "std      870.759412        227.142709     278.699913     24.726966   \n",
      "min        1.000000          1.000000       0.000000      0.000000   \n",
      "25%       65.000000         50.000000      18.000000      0.000000   \n",
      "50%      155.000000        100.000000      47.000000      0.000000   \n",
      "75%      345.000000        178.000000     103.000000      1.000000   \n",
      "max    23343.000000       7700.000000    7338.000000   3133.000000   \n",
      "\n",
      "       num_unique_domains  num_email_addresses  num_spelling_errors  \\\n",
      "count        16306.000000         16306.000000         16306.000000   \n",
      "mean             0.627008             0.849565             5.772415   \n",
      "std             18.843671             6.219692             5.685461   \n",
      "min              0.000000             0.000000             0.000000   \n",
      "25%              0.000000             0.000000             2.000000   \n",
      "50%              0.000000             0.000000             5.000000   \n",
      "75%              1.000000             0.000000             8.000000   \n",
      "max           2401.000000           495.000000            77.000000   \n",
      "\n",
      "       num_urgent_keywords  \n",
      "count         16306.000000  \n",
      "mean              0.504232  \n",
      "std               2.052668  \n",
      "min               0.000000  \n",
      "25%               0.000000  \n",
      "50%               0.000000  \n",
      "75%               0.000000  \n",
      "max              76.000000  \n",
      "\n",
      "======================================================================\n",
      "PHISHING EMAILS (label=1)\n",
      "======================================================================\n",
      "          num_words  num_unique_words  num_stopwords    num_links  \\\n",
      "count   7303.000000       7303.000000    7303.000000  7303.000000   \n",
      "mean     299.364645        137.682185      87.616596     0.277557   \n",
      "std      551.757135        159.392584     186.980592     1.456781   \n",
      "min        1.000000          1.000000       0.000000     0.000000   \n",
      "25%       68.000000         52.000000      13.000000     0.000000   \n",
      "50%      136.000000         88.000000      33.000000     0.000000   \n",
      "75%      290.000000        158.000000      81.000000     0.000000   \n",
      "max    11450.000000       2876.000000    4801.000000    64.000000   \n",
      "\n",
      "       num_unique_domains  num_email_addresses  num_spelling_errors  \\\n",
      "count         7303.000000          7303.000000          7303.000000   \n",
      "mean             0.191291             0.154731             6.680268   \n",
      "std              0.812441             0.791037             9.254108   \n",
      "min              0.000000             0.000000             0.000000   \n",
      "25%              0.000000             0.000000             2.000000   \n",
      "50%              0.000000             0.000000             4.000000   \n",
      "75%              0.000000             0.000000             8.000000   \n",
      "max             34.000000            26.000000            89.000000   \n",
      "\n",
      "       num_urgent_keywords  \n",
      "count          7303.000000  \n",
      "mean              0.776530  \n",
      "std               1.869375  \n",
      "min               0.000000  \n",
      "25%               0.000000  \n",
      "50%               0.000000  \n",
      "75%               1.000000  \n",
      "max              57.000000  \n"
     ]
    }
   ],
   "source": [
    "feature_cols = [\n",
    "        'num_words', 'num_unique_words', 'num_stopwords', 'num_links',\n",
    "        'num_unique_domains', 'num_email_addresses', 'num_spelling_errors',\n",
    "        'num_urgent_keywords'\n",
    "    ]\n",
    "\n",
    "print(\"\\nðŸ“Š Feature statistics by class:\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LEGITIMATE EMAILS (label=0)\")\n",
    "print(\"=\"*70)\n",
    "print(combined_df[combined_df['label'] == 0][feature_cols].describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHISHING EMAILS (label=1)\")\n",
    "print(\"=\"*70)\n",
    "print(combined_df[combined_df['label'] == 1][feature_cols].describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
