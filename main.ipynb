{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c18deb6",
   "metadata": {},
   "source": [
    "# Phishing Email Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a694136",
   "metadata": {},
   "source": [
    "## Dataset Download and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a22b09f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset from Kaggle...\n",
      "Dataset URL: https://www.kaggle.com/datasets/subhajournal/phishingemails\n",
      "License(s): GNU Lesser General Public License 3.0\n",
      "Downloading phishingemails.zip to data/raw\n",
      "  0%|                                               | 0.00/18.0M [00:00<?, ?B/s]\n",
      "100%|███████████████████████████████████████| 18.0M/18.0M [00:00<00:00, 875MB/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Directory to save the downloaded dataset\n",
    "DATA_RAW_DIR = Path(\"data/raw\")\n",
    "\n",
    "# Make sure the directory exists\n",
    "DATA_RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATASET_NAME=\"subhajournal/phishingemails\" # naserabdullahalam/phishing-email-dataset\n",
    "\n",
    "# Check if datsets are already downloaded\n",
    "if not any(DATA_RAW_DIR.iterdir()):\n",
    "    print(\"Downloading dataset from Kaggle...\")\n",
    "    # Download the dataset from Kaggle\n",
    "    !kaggle datasets download -d {DATASET_NAME} -p {DATA_RAW_DIR} --unzip\n",
    "else :\n",
    "    print(\"Dataset already exists. Skipping download.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2095ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9801884a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a49654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/enrico/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from urlextract import URLExtract\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "# Config (adjust these as you need)\n",
    "DATA_CSV = \"emails.csv\" # path to your CSV\n",
    "MAX_VOCAB = 40000\n",
    "MAX_LEN = 256\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 6\n",
    "LR = 1e-3\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad39a8c",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379769ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = URLExtract()\n",
    "\n",
    "\n",
    "def clean_html(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text(separator=\" \")\n",
    "\n",
    "\n",
    "def remove_urls(text):\n",
    "    urls = extractor.find_urls(text)\n",
    "    for u in urls:\n",
    "        text = text.replace(u, \" URLTOKEN \")\n",
    "    return text, len(urls), urls\n",
    "\n",
    "\n",
    "def simple_preprocess(text):\n",
    "    # collapse whitespace, lowercase\n",
    "    text = re.sub(r\"\\s+\", \" \", str(text))\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\n",
    "# Small keyword feature helper\n",
    "KEYWORD_PATTERN = re.compile(\n",
    "    r\"\\b(password|verify|account|bank|login|confirm|click|urgent|reset)\\b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d032d1a",
   "metadata": {},
   "source": [
    "## Vocabulary builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e31cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(token_lists, max_vocab=MAX_VOCAB, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for toks in token_lists:\n",
    "        counter.update(toks)\n",
    "    vocab_tokens = [\"<PAD>\", \"<UNK>\"] + [\n",
    "        tok for tok, c in counter.most_common(max_vocab) if c >= min_freq\n",
    "    ]\n",
    "    stoi = {tok: i for i, tok in enumerate(vocab_tokens)}\n",
    "    itos = {i: tok for tok, i in stoi.items()}\n",
    "    return stoi, itos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052e8933",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0d1785",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, df, stoi, scaler=None, max_len=MAX_LEN):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.stoi = stoi\n",
    "        self.max_len = max_len\n",
    "        self.scaler = scaler\n",
    "        self.url_extractor = URLExtract()\n",
    "\n",
    "        # Precompute numeric features\n",
    "        self.num_feats = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            subj = row.get(\"subject\", \"\") or \"\"\n",
    "            body = row.get(\"body\", \"\") or \"\"\n",
    "            raw = subj + \" \" + body\n",
    "            text = clean_html(raw)\n",
    "            text = simple_preprocess(text)\n",
    "            text, n_urls, urls = remove_urls(text)\n",
    "\n",
    "            n_upper = sum(1 for c in (subj + body) if c.isupper())\n",
    "            n_exclaim = (subj + body).count(\"!\")\n",
    "            n_special = sum(\n",
    "                1 for c in (subj + body) if not c.isalnum() and not c.isspace()\n",
    "            )\n",
    "            length = len(text.split())\n",
    "            has_login_words = int(bool(KEYWORD_PATTERN.search(text)))\n",
    "            features = [n_urls, n_upper, n_exclaim, n_special, length, has_login_words]\n",
    "            self.num_feats.append(features)\n",
    "\n",
    "        self.num_feats = np.array(self.num_feats, dtype=np.float32)\n",
    "        if scaler is not None:\n",
    "            self.num_feats = scaler.transform(self.num_feats)\n",
    "\n",
    "        # Tokenize to ids\n",
    "        self.seq_ids = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            subj = row.get(\"subject\", \"\") or \"\"\n",
    "            body = row.get(\"body\", \"\") or \"\"\n",
    "            text = clean_html(subj + \" \" + body)\n",
    "            text = simple_preprocess(text)\n",
    "            text, _, _ = remove_urls(text)\n",
    "            toks = tokenize(text)\n",
    "            ids = [self.stoi.get(tok, self.stoi.get(\"<UNK>\")) for tok in toks][\n",
    "                : self.max_len\n",
    "            ]\n",
    "            self.seq_ids.append(ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.seq_ids[idx]\n",
    "        num = self.num_feats[idx].astype(np.float32)\n",
    "        label = int(self.df.loc[idx, \"label\"])\n",
    "        return {\n",
    "            \"seq\": torch.tensor(seq, dtype=torch.long),\n",
    "            \"num\": torch.tensor(num, dtype=torch.float),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def collate_batch(batch):\n",
    "        seqs = [item[\"seq\"] for item in batch]\n",
    "        lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "        maxlen = max(lengths).item()\n",
    "        padded = torch.zeros(len(seqs), maxlen, dtype=torch.long)\n",
    "        for i, s in enumerate(seqs):\n",
    "            padded[i, : len(s)] = s\n",
    "        nums = torch.stack([item[\"num\"] for item in batch])\n",
    "        labels = torch.tensor([item[\"label\"] for item in batch], dtype=torch.long)\n",
    "        return {\"seq\": padded, \"lengths\": lengths, \"num\": nums, \"label\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac15050",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d6fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: batch x seq x dim\n",
    "        scores = self.proj(x).squeeze(-1) # batch x seq\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1).unsqueeze(-1) # batch x seq x 1\n",
    "        out = (x * attn).sum(dim=1) # batch x dim\n",
    "        return out, attn\n",
    "\n",
    "\n",
    "class PhishDetector(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=EMBED_DIM, hidden_dim=HIDDEN_DIM, num_feats_dim=6, num_classes=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.att = Attention(hidden_dim * 2)\n",
    "        self.fc_text = nn.Linear(hidden_dim * 2, 128)\n",
    "        self.fc_comb = nn.Linear(128 + num_feats_dim, 64)\n",
    "        self.out = nn.Linear(64, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, seq, lengths, num_feats):\n",
    "        emb = self.embedding(seq)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        mask = (seq != 0).to(seq.device)\n",
    "        attn_out, attn_weights = self.att(out, mask)\n",
    "        x = F.relu(self.fc_text(attn_out))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.cat([x, num_feats], dim=1)\n",
    "        x = F.relu(self.fc_comb(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.out(x)\n",
    "        return logits, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a82407",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87025d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred_probs, threshold=0.5):\n",
    "    y_pred = (y_pred_probs[:, 1] >= threshold).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_pred_probs[:, 1])\n",
    "    except Exception:\n",
    "        auc = float(\"nan\")\n",
    "    return {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f1, \"roc_auc\": auc}\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, opt, criterion):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch in tqdm(dataloader, desc=\"train step\"):\n",
    "        seq = batch[\"seq\"].to(DEVICE)\n",
    "        lengths = batch[\"lengths\"].to(DEVICE)\n",
    "        num = batch[\"num\"].to(DEVICE)\n",
    "        labels = batch[\"label\"].to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        logits, _ = model(seq, lengths, num)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def eval_model(model, dataloader):\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            seq = batch[\"seq\"].to(DEVICE)\n",
    "            lengths = batch[\"lengths\"].to(DEVICE)\n",
    "            num = batch[\"num\"].to(DEVICE)\n",
    "            labels = batch[\"label\"].to(DEVICE)\n",
    "            logits, attn = model(seq, lengths, num)\n",
    "            p = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "            probs.append(p)\n",
    "            trues.append(labels.cpu().numpy())\n",
    "    probs = np.vstack(probs)\n",
    "    trues = np.concatenate(trues)\n",
    "    return trues, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f7ad79",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d4bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(DATA_CSV), f\"CSV file not found: {DATA_CSV}\"\n",
    "df[\"body\"] = df[\"body\"].astype(str)\n",
    "\n",
    "\n",
    "# Create token lists for vocab building (train-only later)\n",
    "token_lists = []\n",
    "for _, row in df.iterrows():\n",
    "    text = clean_html(row[\"subject\"] + \" \" + row[\"body\"])\n",
    "    text = simple_preprocess(text)\n",
    "    text, _, _ = remove_urls(text)\n",
    "    toks = tokenize(text)\n",
    "    token_lists.append(toks)\n",
    "\n",
    "\n",
    "# Split\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.15, stratify=df[\"label\"], random_state=SEED\n",
    ")\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df, test_size=0.1, stratify=train_df[\"label\"], random_state=SEED\n",
    ")\n",
    "\n",
    "\n",
    "# Build vocab on train only\n",
    "train_token_lists = [token_lists[i] for i in train_df.index]\n",
    "stoi, itos = build_vocab(train_token_lists, max_vocab=MAX_VOCAB)\n",
    "vocab_size = len(stoi)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "\n",
    "# Fit scaler on train\n",
    "train_ds_tmp = EmailDataset(train_df, stoi, scaler=None)\n",
    "scaler = StandardScaler().fit(train_ds_tmp.num_feats)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_ds = EmailDataset(train_df, stoi, scaler=scaler)\n",
    "val_ds = EmailDataset(val_df, stoi, scaler=scaler)\n",
    "test_ds = EmailDataset(test_df, stoi, scaler=scaler)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "\n",
    "# Weighted loss for imbalance\n",
    "label_counts = train_df[\"label\"].value_counts().sort_index()\n",
    "total = label_counts.sum()\n",
    "weights = [total / (2 * c) for c in label_counts]\n",
    "class_weights = torch.tensor(weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = PhishDetector(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_feats_dim=train_ds.num_feats.shape[1],\n",
    ")\n",
    "model.to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "best_val_f1 = -1\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    train_loss = train_one_epoch(model, train_loader, opt, criterion)\n",
    "    trues_val, probs_val = eval_model(model, val_loader)\n",
    "    val_metrics = compute_metrics(trues_val, probs_val)\n",
    "    print(f\"Train loss: {train_loss:.4f} Val metrics: {val_metrics}\")\n",
    "    if val_metrics[\"f1\"] > best_val_f1:\n",
    "        best_val_f1 = val_metrics[\"f1\"]\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"stoi\": stoi,\n",
    "                \"itos\": itos,\n",
    "                \"scaler\": scaler,\n",
    "            },\n",
    "            \"best_phish_model.pth\",\n",
    "        )\n",
    "        print(\"Saved best model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce4dc4d",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc7c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"best_phish_model.pth\", map_location=DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "trues_test, probs_test = eval_model(model, test_loader)\n",
    "test_metrics = compute_metrics(trues_test, probs_test)\n",
    "print(\"Final test metrics:\", test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5ff22",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf38476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_artifacts(filepath=\"best_phish_model.pth\"):\n",
    "    ckpt = torch.load(filepath, map_location=DEVICE)\n",
    "    model = PhishDetector(\n",
    "        vocab_size=len(ckpt[\"stoi\"]),\n",
    "        emb_dim=EMBED_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        num_feats_dim=6,\n",
    "    )\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return model, ckpt[\"stoi\"], ckpt[\"scaler\"]\n",
    "\n",
    "\n",
    "def preprocess_single(subject, body, stoi, scaler, max_len=MAX_LEN):\n",
    "    raw = (subject or \"\") + \" \" + (body or \"\")\n",
    "    text = clean_html(raw)\n",
    "    text = simple_preprocess(text)\n",
    "    text, n_urls, urls = remove_urls(text)\n",
    "    toks = tokenize(text)\n",
    "    ids = [stoi.get(tok, stoi.get(\"<UNK>\")) for tok in toks][:max_len]\n",
    "\n",
    "    # numeric features\n",
    "    n_upper = sum(1 for c in raw if c.isupper())\n",
    "    n_exclaim = raw.count(\"!\")\n",
    "    n_special = sum(1 for c in raw if not c.isalnum() and not c.isspace())\n",
    "    length = len(text.split())\n",
    "    has_login_words = int(bool(KEYWORD_PATTERN.search(text)))\n",
    "    feats = np.array(\n",
    "        [[n_urls, n_upper, n_exclaim, n_special, length, has_login_words]],\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "    feats = scaler.transform(feats)\n",
    "\n",
    "    seq = torch.tensor([ids], dtype=torch.long).to(DEVICE)\n",
    "    lengths = torch.tensor([len(ids)], dtype=torch.long).to(DEVICE)\n",
    "    num = torch.tensor(feats, dtype=torch.float).to(DEVICE)\n",
    "    return seq, lengths, num\n",
    "\n",
    "\n",
    "def predict_email(subject, body, model, stoi, scaler, threshold=0.5):\n",
    "    seq, lengths, num = preprocess_single(subject, body, stoi, scaler)\n",
    "    with torch.no_grad():\n",
    "        logits, attn = model(seq, lengths, num)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "    label = int(probs[1] >= threshold)\n",
    "    return {\n",
    "        \"prob_safe\": float(probs[0]),\n",
    "        \"prob_phish\": float(probs[1]),\n",
    "        \"label\": label,\n",
    "        \"attn_weights\": attn.cpu().numpy() if attn is not None else None,\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
