{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c18deb6",
   "metadata": {},
   "source": [
    "# Phishing Email Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a694136",
   "metadata": {},
   "source": [
    "## Dataset Download and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2095ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "MERGED_OUT = DATA_DIR / \"merged_emails.csv\"\n",
    "\n",
    "\n",
    "# helper to try reading different file formats\n",
    "def try_read_file(path):\n",
    "    path = Path(path)\n",
    "    suffix = path.suffix.lower()\n",
    "    try:\n",
    "        if suffix in [\".csv\", \".txt\"]:\n",
    "            # try some common encodings / separators if default fails\n",
    "            return pd.read_csv(path, encoding=\"utf-8\", low_memory=False)\n",
    "        elif suffix in [\".xls\", \".xlsx\"]:\n",
    "            return pd.read_excel(path)\n",
    "        elif suffix == \".json\":\n",
    "            try:\n",
    "                return pd.read_json(path, lines=True)\n",
    "            except Exception:\n",
    "                return pd.read_json(path)\n",
    "        else:\n",
    "            # fallback: read as text into single-column df\n",
    "            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                txt = f.read()\n",
    "            return pd.DataFrame({\"text\": [txt]})\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# find files in data dir\n",
    "all_files = [p for p in DATA_DIR.glob(\"*\") if p.is_file()]\n",
    "print(\"Files found in data/:\", [p.name for p in all_files])\n",
    "\n",
    "# patterns for body/text and label/target columns\n",
    "body_patterns = re.compile(r\"%(body|text|content|email|message|subject)%\", flags=re.I)\n",
    "label_patterns = re.compile(\n",
    "    r\"%(label|class|target|is_phish|spam|type|category)%\", flags=re.I\n",
    ")\n",
    "\n",
    "frames = []\n",
    "for p in all_files:\n",
    "    df = try_read_file(p)\n",
    "    if df is None:\n",
    "        continue\n",
    "    # lower column names\n",
    "    orig_cols = list(df.columns)\n",
    "    cols_lower = [c.lower() if isinstance(c, str) else c for c in orig_cols]\n",
    "    colmap = {orig: orig for orig in orig_cols}\n",
    "\n",
    "    # find body column\n",
    "    body_col = None\n",
    "    for orig, low in zip(orig_cols, cols_lower):\n",
    "        if isinstance(low, str) and body_patterns.search(low):\n",
    "            body_col = orig\n",
    "            break\n",
    "    # fallback: if single column, use it\n",
    "    if body_col is None and len(orig_cols) == 1:\n",
    "        body_col = orig_cols[0]\n",
    "    # find label column\n",
    "    label_col = None\n",
    "    for orig, low in zip(orig_cols, cols_lower):\n",
    "        if isinstance(low, str) and label_patterns.search(low):\n",
    "            label_col = orig\n",
    "            break\n",
    "    # If label not found, try to infer from filename (common in some datasets)\n",
    "    if label_col is None:\n",
    "        fname = p.stem.lower()\n",
    "        if \"phish\" in fname or \"spam\" in fname or \"malicious\" in fname:\n",
    "            # some datasets put only phishing examples in a file -> mark label=1\n",
    "            label_inferred = 1\n",
    "        elif \"ham\" in fname or \"legit\" in fname or \"legitimate\" in fname:\n",
    "            label_inferred = 0\n",
    "        else:\n",
    "            label_inferred = None\n",
    "    else:\n",
    "        label_inferred = None\n",
    "\n",
    "    # build standardized DataFrame with columns: id, subject(optional), body, label\n",
    "    std = pd.DataFrame()\n",
    "    # try to find subject column too\n",
    "    subject_col = None\n",
    "    for orig, low in zip(orig_cols, cols_lower):\n",
    "        if isinstance(low, str) and re.search(r\"%(subject)%\", low):\n",
    "            subject_col = orig\n",
    "            break\n",
    "\n",
    "    # extract body\n",
    "    if body_col is not None:\n",
    "        std[\"body\"] = df[body_col].astype(str)\n",
    "    else:\n",
    "        # try concatenating common text-like columns\n",
    "        text_cols = [\n",
    "            c\n",
    "            for c in orig_cols\n",
    "            if isinstance(c, str)\n",
    "            and (\"body\" in c.lower() or \"text\" in c.lower() or \"message\" in c.lower())\n",
    "        ]\n",
    "        if text_cols:\n",
    "            std[\"body\"] = df[text_cols[0]].astype(str)\n",
    "        else:\n",
    "            # fallback: join all string columns\n",
    "            str_cols = [c for c in orig_cols if df[c].dtype == \"object\"]\n",
    "            if str_cols:\n",
    "                std[\"body\"] = df[str_cols].astype(str).agg(\" \".join, axis=1)\n",
    "            else:\n",
    "                # give up for this file\n",
    "                print(f\"Could not find a body/text column in {p.name}; skipping\")\n",
    "                continue\n",
    "\n",
    "    if subject_col is not None:\n",
    "        std[\"subject\"] = df[subject_col].astype(str)\n",
    "    else:\n",
    "        std[\"subject\"] = \"\"\n",
    "\n",
    "    # extract label\n",
    "    if label_col is not None:\n",
    "        std[\"raw_label\"] = df[label_col]\n",
    "    elif label_inferred is not None:\n",
    "        std[\"raw_label\"] = label_inferred\n",
    "    else:\n",
    "        # no label; try to infer: if file only contains phishing, label=1; else leave NaN\n",
    "        std[\"raw_label\"] = np.nan\n",
    "\n",
    "    # keep provenance\n",
    "    std[\"source_file\"] = p.name\n",
    "    frames.append(std)\n",
    "\n",
    "# concat all frames\n",
    "if not frames:\n",
    "    raise RuntimeError(\"No readable dataset files found in data/\")\n",
    "merged = pd.concat(frames, ignore_index=True)\n",
    "print(\"Merged shape:\", merged.shape)\n",
    "\n",
    "\n",
    "# normalize raw_label into binary label column\n",
    "def normalize_label(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    # numeric\n",
    "    try:\n",
    "        xi = int(x)\n",
    "        if xi in (0, 1):\n",
    "            return xi\n",
    "        # else treat non-zero as phishing\n",
    "        return 1 if xi != 0 else 0\n",
    "    except Exception:\n",
    "        pass\n",
    "    s = str(x).strip().lower()\n",
    "    if s in [\"\", \"nan\", \"none\"]:\n",
    "        return np.nan\n",
    "    if any(\n",
    "        k in s for k in [\"phish\", \"phishing\", \"malicious\", \"spam\", \"1\", \"true\", \"yes\"]\n",
    "    ):\n",
    "        return 1\n",
    "    if any(\n",
    "        k in s\n",
    "        for k in [\"ham\", \"legit\", \"legitimate\", \"safe\", \"not spam\", \"0\", \"false\", \"no\"]\n",
    "    ):\n",
    "        return 0\n",
    "    # try mapping common labels\n",
    "    if s in [\"phishing\", \"phish\", \"spam\", \"malicious\"]:\n",
    "        return 1\n",
    "    if s in [\"ham\", \"legit\", \"legitimate\", \"safe\"]:\n",
    "        return 0\n",
    "    # if contains words like 'phish' treat as 1\n",
    "    if \"phish\" in s or \"spam\" in s or \"malicious\" in s:\n",
    "        return 1\n",
    "    # unknown\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "merged[\"label\"] = merged[\"raw_label\"].apply(normalize_label)\n",
    "\n",
    "# drop rows without body or without label (optional â€” we keep unlabeled rows separately)\n",
    "labeled = merged[merged[\"label\"].notna()].copy()\n",
    "unlabeled = merged[merged[\"label\"].isna()].copy()\n",
    "print(\"Labeled count:\", len(labeled), \"Unlabeled count:\", len(unlabeled))\n",
    "\n",
    "# save merged labeled dataset\n",
    "labeled.to_csv(MERGED_OUT, index=False)\n",
    "print(\"Saved merged labeled dataset to\", MERGED_OUT)\n",
    "\n",
    "# show class distribution\n",
    "print(labeled[\"label\"].value_counts())\n",
    "\n",
    "# quick preview\n",
    "labeled.sample(min(5, len(labeled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9801884a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a49654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/enrico/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from urlextract import URLExtract\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "# Config (adjust these as you need)\n",
    "DATA_CSV = \"emails.csv\" # path to your CSV\n",
    "MAX_VOCAB = 40000\n",
    "MAX_LEN = 256\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 6\n",
    "LR = 1e-3\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad39a8c",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379769ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = URLExtract()\n",
    "\n",
    "\n",
    "def clean_html(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text(separator=\" \")\n",
    "\n",
    "\n",
    "def remove_urls(text):\n",
    "    urls = extractor.find_urls(text)\n",
    "    for u in urls:\n",
    "        text = text.replace(u, \" URLTOKEN \")\n",
    "    return text, len(urls), urls\n",
    "\n",
    "\n",
    "def simple_preprocess(text):\n",
    "    # collapse whitespace, lowercase\n",
    "    text = re.sub(r\"\\s+\", \" \", str(text))\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\n",
    "# Small keyword feature helper\n",
    "KEYWORD_PATTERN = re.compile(\n",
    "    r\"\\b(password|verify|account|bank|login|confirm|click|urgent|reset)\\b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d032d1a",
   "metadata": {},
   "source": [
    "## Vocabulary builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e31cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(token_lists, max_vocab=MAX_VOCAB, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for toks in token_lists:\n",
    "        counter.update(toks)\n",
    "    vocab_tokens = [\"<PAD>\", \"<UNK>\"] + [\n",
    "        tok for tok, c in counter.most_common(max_vocab) if c >= min_freq\n",
    "    ]\n",
    "    stoi = {tok: i for i, tok in enumerate(vocab_tokens)}\n",
    "    itos = {i: tok for tok, i in stoi.items()}\n",
    "    return stoi, itos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052e8933",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0d1785",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, df, stoi, scaler=None, max_len=MAX_LEN):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.stoi = stoi\n",
    "        self.max_len = max_len\n",
    "        self.scaler = scaler\n",
    "        self.url_extractor = URLExtract()\n",
    "\n",
    "        # Precompute numeric features\n",
    "        self.num_feats = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            subj = row.get(\"subject\", \"\") or \"\"\n",
    "            body = row.get(\"body\", \"\") or \"\"\n",
    "            raw = subj + \" \" + body\n",
    "            text = clean_html(raw)\n",
    "            text = simple_preprocess(text)\n",
    "            text, n_urls, urls = remove_urls(text)\n",
    "\n",
    "            n_upper = sum(1 for c in (subj + body) if c.isupper())\n",
    "            n_exclaim = (subj + body).count(\"!\")\n",
    "            n_special = sum(\n",
    "                1 for c in (subj + body) if not c.isalnum() and not c.isspace()\n",
    "            )\n",
    "            length = len(text.split())\n",
    "            has_login_words = int(bool(KEYWORD_PATTERN.search(text)))\n",
    "            features = [n_urls, n_upper, n_exclaim, n_special, length, has_login_words]\n",
    "            self.num_feats.append(features)\n",
    "\n",
    "        self.num_feats = np.array(self.num_feats, dtype=np.float32)\n",
    "        if scaler is not None:\n",
    "            self.num_feats = scaler.transform(self.num_feats)\n",
    "\n",
    "        # Tokenize to ids\n",
    "        self.seq_ids = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            subj = row.get(\"subject\", \"\") or \"\"\n",
    "            body = row.get(\"body\", \"\") or \"\"\n",
    "            text = clean_html(subj + \" \" + body)\n",
    "            text = simple_preprocess(text)\n",
    "            text, _, _ = remove_urls(text)\n",
    "            toks = tokenize(text)\n",
    "            ids = [self.stoi.get(tok, self.stoi.get(\"<UNK>\")) for tok in toks][\n",
    "                : self.max_len\n",
    "            ]\n",
    "            self.seq_ids.append(ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.seq_ids[idx]\n",
    "        num = self.num_feats[idx].astype(np.float32)\n",
    "        label = int(self.df.loc[idx, \"label\"])\n",
    "        return {\n",
    "            \"seq\": torch.tensor(seq, dtype=torch.long),\n",
    "            \"num\": torch.tensor(num, dtype=torch.float),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def collate_batch(batch):\n",
    "        seqs = [item[\"seq\"] for item in batch]\n",
    "        lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "        maxlen = max(lengths).item()\n",
    "        padded = torch.zeros(len(seqs), maxlen, dtype=torch.long)\n",
    "        for i, s in enumerate(seqs):\n",
    "            padded[i, : len(s)] = s\n",
    "        nums = torch.stack([item[\"num\"] for item in batch])\n",
    "        labels = torch.tensor([item[\"label\"] for item in batch], dtype=torch.long)\n",
    "        return {\"seq\": padded, \"lengths\": lengths, \"num\": nums, \"label\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac15050",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d6fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: batch x seq x dim\n",
    "        scores = self.proj(x).squeeze(-1) # batch x seq\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1).unsqueeze(-1) # batch x seq x 1\n",
    "        out = (x * attn).sum(dim=1) # batch x dim\n",
    "        return out, attn\n",
    "\n",
    "\n",
    "class PhishDetector(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=EMBED_DIM, hidden_dim=HIDDEN_DIM, num_feats_dim=6, num_classes=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.att = Attention(hidden_dim * 2)\n",
    "        self.fc_text = nn.Linear(hidden_dim * 2, 128)\n",
    "        self.fc_comb = nn.Linear(128 + num_feats_dim, 64)\n",
    "        self.out = nn.Linear(64, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, seq, lengths, num_feats):\n",
    "        emb = self.embedding(seq)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        mask = (seq != 0).to(seq.device)\n",
    "        attn_out, attn_weights = self.att(out, mask)\n",
    "        x = F.relu(self.fc_text(attn_out))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.cat([x, num_feats], dim=1)\n",
    "        x = F.relu(self.fc_comb(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.out(x)\n",
    "        return logits, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a82407",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87025d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred_probs, threshold=0.5):\n",
    "    y_pred = (y_pred_probs[:, 1] >= threshold).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_pred_probs[:, 1])\n",
    "    except Exception:\n",
    "        auc = float(\"nan\")\n",
    "    return {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f1, \"roc_auc\": auc}\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, opt, criterion):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch in tqdm(dataloader, desc=\"train step\"):\n",
    "        seq = batch[\"seq\"].to(DEVICE)\n",
    "        lengths = batch[\"lengths\"].to(DEVICE)\n",
    "        num = batch[\"num\"].to(DEVICE)\n",
    "        labels = batch[\"label\"].to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        logits, _ = model(seq, lengths, num)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def eval_model(model, dataloader):\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            seq = batch[\"seq\"].to(DEVICE)\n",
    "            lengths = batch[\"lengths\"].to(DEVICE)\n",
    "            num = batch[\"num\"].to(DEVICE)\n",
    "            labels = batch[\"label\"].to(DEVICE)\n",
    "            logits, attn = model(seq, lengths, num)\n",
    "            p = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "            probs.append(p)\n",
    "            trues.append(labels.cpu().numpy())\n",
    "    probs = np.vstack(probs)\n",
    "    trues = np.concatenate(trues)\n",
    "    return trues, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f7ad79",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d4bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(DATA_CSV), f\"CSV file not found: {DATA_CSV}\"\n",
    "df[\"body\"] = df[\"body\"].astype(str)\n",
    "\n",
    "\n",
    "# Create token lists for vocab building (train-only later)\n",
    "token_lists = []\n",
    "for _, row in df.iterrows():\n",
    "    text = clean_html(row[\"subject\"] + \" \" + row[\"body\"])\n",
    "    text = simple_preprocess(text)\n",
    "    text, _, _ = remove_urls(text)\n",
    "    toks = tokenize(text)\n",
    "    token_lists.append(toks)\n",
    "\n",
    "\n",
    "# Split\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.15, stratify=df[\"label\"], random_state=SEED\n",
    ")\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df, test_size=0.1, stratify=train_df[\"label\"], random_state=SEED\n",
    ")\n",
    "\n",
    "\n",
    "# Build vocab on train only\n",
    "train_token_lists = [token_lists[i] for i in train_df.index]\n",
    "stoi, itos = build_vocab(train_token_lists, max_vocab=MAX_VOCAB)\n",
    "vocab_size = len(stoi)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "\n",
    "# Fit scaler on train\n",
    "train_ds_tmp = EmailDataset(train_df, stoi, scaler=None)\n",
    "scaler = StandardScaler().fit(train_ds_tmp.num_feats)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_ds = EmailDataset(train_df, stoi, scaler=scaler)\n",
    "val_ds = EmailDataset(val_df, stoi, scaler=scaler)\n",
    "test_ds = EmailDataset(test_df, stoi, scaler=scaler)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "\n",
    "# Weighted loss for imbalance\n",
    "label_counts = train_df[\"label\"].value_counts().sort_index()\n",
    "total = label_counts.sum()\n",
    "weights = [total / (2 * c) for c in label_counts]\n",
    "class_weights = torch.tensor(weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = PhishDetector(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_feats_dim=train_ds.num_feats.shape[1],\n",
    ")\n",
    "model.to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "best_val_f1 = -1\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    train_loss = train_one_epoch(model, train_loader, opt, criterion)\n",
    "    trues_val, probs_val = eval_model(model, val_loader)\n",
    "    val_metrics = compute_metrics(trues_val, probs_val)\n",
    "    print(f\"Train loss: {train_loss:.4f} Val metrics: {val_metrics}\")\n",
    "    if val_metrics[\"f1\"] > best_val_f1:\n",
    "        best_val_f1 = val_metrics[\"f1\"]\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"stoi\": stoi,\n",
    "                \"itos\": itos,\n",
    "                \"scaler\": scaler,\n",
    "            },\n",
    "            \"best_phish_model.pth\",\n",
    "        )\n",
    "        print(\"Saved best model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce4dc4d",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc7c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"best_phish_model.pth\", map_location=DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "trues_test, probs_test = eval_model(model, test_loader)\n",
    "test_metrics = compute_metrics(trues_test, probs_test)\n",
    "print(\"Final test metrics:\", test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5ff22",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf38476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_artifacts(filepath=\"best_phish_model.pth\"):\n",
    "    ckpt = torch.load(filepath, map_location=DEVICE)\n",
    "    model = PhishDetector(\n",
    "        vocab_size=len(ckpt[\"stoi\"]),\n",
    "        emb_dim=EMBED_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        num_feats_dim=6,\n",
    "    )\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return model, ckpt[\"stoi\"], ckpt[\"scaler\"]\n",
    "\n",
    "\n",
    "def preprocess_single(subject, body, stoi, scaler, max_len=MAX_LEN):\n",
    "    raw = (subject or \"\") + \" \" + (body or \"\")\n",
    "    text = clean_html(raw)\n",
    "    text = simple_preprocess(text)\n",
    "    text, n_urls, urls = remove_urls(text)\n",
    "    toks = tokenize(text)\n",
    "    ids = [stoi.get(tok, stoi.get(\"<UNK>\")) for tok in toks][:max_len]\n",
    "\n",
    "    # numeric features\n",
    "    n_upper = sum(1 for c in raw if c.isupper())\n",
    "    n_exclaim = raw.count(\"!\")\n",
    "    n_special = sum(1 for c in raw if not c.isalnum() and not c.isspace())\n",
    "    length = len(text.split())\n",
    "    has_login_words = int(bool(KEYWORD_PATTERN.search(text)))\n",
    "    feats = np.array(\n",
    "        [[n_urls, n_upper, n_exclaim, n_special, length, has_login_words]],\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "    feats = scaler.transform(feats)\n",
    "\n",
    "    seq = torch.tensor([ids], dtype=torch.long).to(DEVICE)\n",
    "    lengths = torch.tensor([len(ids)], dtype=torch.long).to(DEVICE)\n",
    "    num = torch.tensor(feats, dtype=torch.float).to(DEVICE)\n",
    "    return seq, lengths, num\n",
    "\n",
    "\n",
    "def predict_email(subject, body, model, stoi, scaler, threshold=0.5):\n",
    "    seq, lengths, num = preprocess_single(subject, body, stoi, scaler)\n",
    "    with torch.no_grad():\n",
    "        logits, attn = model(seq, lengths, num)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "    label = int(probs[1] >= threshold)\n",
    "    return {\n",
    "        \"prob_safe\": float(probs[0]),\n",
    "        \"prob_phish\": float(probs[1]),\n",
    "        \"label\": label,\n",
    "        \"attn_weights\": attn.cpu().numpy() if attn is not None else None,\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
