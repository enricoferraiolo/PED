{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c18deb6",
   "metadata": {},
   "source": [
    "# Phishing Email Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9801884a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a49654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# For text tokenization\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Styling\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "DATA_PATH = Path(\"data/preprocessed/emails_combined.csv\")\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Feature columns for ML models\n",
    "FEATURE_COLS = [\n",
    "    'num_words', 'num_unique_words', 'num_stopwords', 'num_links',\n",
    "    'num_unique_domains', 'num_email_addresses', 'num_spelling_errors',\n",
    "    'num_urgent_keywords'\n",
    "]\n",
    "\n",
    "# Deep Learning parameters\n",
    "MAX_VOCAB_SIZE = 10000  # Vocabulary size\n",
    "MAX_LEN = 200          # Maximum sequence length\n",
    "EMBEDDING_DIM = 128    # Embedding dimension\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa21e8b8",
   "metadata": {},
   "source": [
    "## Email Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e895630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for email classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, vocab=None, max_len=MAX_LEN):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Build or use existing vocabulary\n",
    "        if vocab is None:\n",
    "            self.vocab = self.build_vocab(texts)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "    \n",
    "    def build_vocab(self, texts):\n",
    "        \"\"\"Build vocabulary from texts.\"\"\"\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            words = self.tokenize(text)\n",
    "            word_counts.update(words)\n",
    "        \n",
    "        # Get most common words\n",
    "        most_common = word_counts.most_common(MAX_VOCAB_SIZE - 2)  # Reserve for PAD and UNK\n",
    "        \n",
    "        # Build vocab dictionary\n",
    "        vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "        for idx, (word, _) in enumerate(most_common, start=2):\n",
    "            vocab[word] = idx\n",
    "        \n",
    "        return vocab\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Simple tokenization.\"\"\"\n",
    "        text = text.lower()\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        return words\n",
    "    \n",
    "    def text_to_sequence(self, text):\n",
    "        \"\"\"Convert text to sequence of indices.\"\"\"\n",
    "        words = self.tokenize(text)\n",
    "        sequence = [self.vocab.get(word, self.vocab['<UNK>']) for word in words]\n",
    "        \n",
    "        # Truncate or pad\n",
    "        if len(sequence) > self.max_len:\n",
    "            sequence = sequence[:self.max_len]\n",
    "        else:\n",
    "            sequence = sequence + [self.vocab['<PAD>']] * (self.max_len - len(sequence))\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        \n",
    "        sequence = self.text_to_sequence(text)\n",
    "        \n",
    "        return torch.tensor(sequence, dtype=torch.long), torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac15050",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb42f27",
   "metadata": {},
   "source": [
    "### DL and ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3812c",
   "metadata": {},
   "source": [
    "#### DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "929d6fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"LSTM model for text classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2, dropout=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, \n",
    "                           batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, 64)  # *2 for bidirectional\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Use last hidden state (concatenate forward and backward)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        out = self.fc1(hidden)\n",
    "        out = torch.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out.squeeze()\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    \"\"\"CNN model for text classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters=128, filter_sizes=[3, 4, 5], dropout=0.5):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Multiple convolutional layers with different filter sizes\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        self.fc1 = nn.Linear(len(filter_sizes) * num_filters, 64)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # Permute for Conv1d: (batch_size, embedding_dim, seq_len)\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        \n",
    "        # Apply convolution and max pooling\n",
    "        conved = [torch.relu(conv(embedded)) for conv in self.convs]\n",
    "        pooled = [torch.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        # Concatenate pooled outputs\n",
    "        cat = torch.cat(pooled, dim=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        out = self.fc1(cat)\n",
    "        out = torch.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out.squeeze()\n",
    "    \n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for sequences, labels in dataloader:\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        total_loss += loss.item()\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in dataloader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            total_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # Store predictions\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(outputs.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, accuracy, np.array(all_labels), np.array(all_preds), np.array(all_probs)\n",
    "\n",
    "def train_dl_models(X_text_train, X_text_test, y_train, y_test):\n",
    "    \"\"\"Train and evaluate deep learning models using PyTorch: LSTM and CNN.\"\"\"\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"Preparing datasets...\")\n",
    "    train_dataset = EmailDataset(X_text_train.reset_index(drop=True), \n",
    "                                 y_train.reset_index(drop=True))\n",
    "    test_dataset = EmailDataset(X_text_test.reset_index(drop=True), \n",
    "                                y_test.reset_index(drop=True),\n",
    "                                vocab=train_dataset.vocab)\n",
    "    \n",
    "    # Create validation split from training data\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        train_dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    print(f\"âœ… Vocabulary size: {len(test_dataset.vocab)}\")\n",
    "    print(f\"âœ… Train samples: {len(train_dataset)}\")\n",
    "    print(f\"âœ… Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"âœ… Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    dl_results = {}\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 1. LSTM Model\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"1. LSTM MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    lstm_model = LSTMClassifier(\n",
    "        vocab_size=len(test_dataset.vocab),\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=64,\n",
    "        num_layers=2,\n",
    "        dropout=0.5\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"Model architecture:\")\n",
    "    print(lstm_model)\n",
    "    \n",
    "    optimizer = optim.Adam(lstm_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    print(f\"Training LSTM for {EPOCHS} epochs...\")\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc = train_epoch(lstm_model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, _, _, _ = evaluate(lstm_model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(lstm_model.state_dict(), RESULTS_DIR / 'best_lstm.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    lstm_model.load_state_dict(torch.load(RESULTS_DIR / 'best_lstm.pth'))\n",
    "    \n",
    "    print(\"\\nâœ… Training complete!\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    _, _, y_true_lstm, y_pred_lstm, y_prob_lstm = evaluate(lstm_model, test_loader, criterion, device)\n",
    "    \n",
    "    dl_results['LSTM'] = evaluate_model(\n",
    "        y_true_lstm, y_pred_lstm, y_prob_lstm, \"LSTM\"\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 2. CNN Model\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"2. CNN MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    cnn_model = CNNClassifier(\n",
    "        vocab_size=len(test_dataset.vocab),\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        num_filters=128,\n",
    "        filter_sizes=[3, 4, 5],\n",
    "        dropout=0.5\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"Model architecture:\")\n",
    "    print(cnn_model)\n",
    "    \n",
    "    optimizer = optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    print(f\"Training CNN for {EPOCHS} epochs...\")\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc = train_epoch(cnn_model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, _, _, _ = evaluate(cnn_model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(cnn_model.state_dict(), RESULTS_DIR / 'best_cnn.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    cnn_model.load_state_dict(torch.load(RESULTS_DIR / 'best_cnn.pth'))\n",
    "    \n",
    "    print(\"\\nâœ… Training complete!\")\n",
    "    print(\"\\nðŸ“Š Evaluating on test set...\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    _, _, y_true_cnn, y_pred_cnn, y_prob_cnn = evaluate(cnn_model, test_loader, criterion, device)\n",
    "    \n",
    "    dl_results['CNN'] = evaluate_model(\n",
    "        y_true_cnn, y_pred_cnn, y_prob_cnn, \"CNN\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return dl_results, (lstm_model, cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645066f5",
   "metadata": {},
   "source": [
    "##### ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5984dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ml_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Train and evaluate ML models: Logistic Regression, Random Forest, XGBoost.\"\"\"\n",
    "   \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    ml_results = {}\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 1. Logistic Regression\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"1. LOGISTIC REGRESSION\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(\"Training Logistic Regression...\")\n",
    "    lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    lr_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    print(\"âœ… Training complete!\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "    y_pred_proba_lr = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    ml_results['Logistic Regression'] = evaluate_model(\n",
    "        y_test, y_pred_lr, y_pred_proba_lr, \"Logistic Regression\"\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 2. Random Forest\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"2. RANDOM FOREST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"Training Random Forest...\")\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        max_depth=20,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"âœ… Training complete!\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    ml_results['Random Forest'] = evaluate_model(\n",
    "        y_test, y_pred_rf, y_pred_proba_rf, \"Random Forest\"\n",
    "    )\n",
    "    \n",
    "    # Feature importance\n",
    "    print(\"Feature Importance (Random Forest):\")\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': FEATURE_COLS,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    print(feature_importance)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 3. XGBoost\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"3. XGBOOST\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(\"Training XGBoost...\")\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"âœ… Training complete!\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    ml_results['XGBoost'] = evaluate_model(\n",
    "        y_test, y_pred_xgb, y_pred_proba_xgb, \"XGBoost\"\n",
    "    )\n",
    "    \n",
    "    # Feature importance\n",
    "    print(\"Feature Importance (XGBoost):\")\n",
    "    feature_importance_xgb = pd.DataFrame({\n",
    "        'feature': FEATURE_COLS,\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    print(feature_importance_xgb)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return ml_results, (lr_model, rf_model, xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a82407",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87025d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_data(test_size=0.2, random_state=42):\n",
    "    \"\"\"Load the preprocessed data and split into train/test sets.\"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    print(f\"Loading data from: {DATA_PATH}\")\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "    # Check for missing values\n",
    "    print(f\"\\nMissing values:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Class distribution\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    print(df['label'].value_counts())\n",
    "    print(f\"Class balance:\")\n",
    "    print(df['label'].value_counts(normalize=True))\n",
    "    \n",
    "    # Split data\n",
    "    print(f\"\\nSplitting data (test_size={test_size})...\")\n",
    "    \n",
    "    X = df[FEATURE_COLS]  # Features for ML\n",
    "    X_text = df['text']    # Text for DL\n",
    "    y = df['label']        # Labels\n",
    "    \n",
    "    # Stratified split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    X_text_train, X_text_test, _, _ = train_test_split(\n",
    "        X_text, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Train set: {X_train.shape[0]} samples\")\n",
    "    print(f\"âœ… Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    return X_train, X_test, X_text_train, X_text_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b255e96",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6561d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, y_pred_proba, model_name):\n",
    "    \"\"\"Evaluate model performance and print metrics.\"\"\"\n",
    "    \n",
    "    print(f\"Evaluation Metrics for {model_name}:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(f\"Confusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Legitimate', 'Phishing']))\n",
    "    \n",
    "    # Store results\n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'confusion_matrix': cm,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df22ed2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: data/preprocessed/emails_combined.csv\n",
      "Dataset shape: (23609, 10)\n",
      "Columns: ['text', 'label', 'num_words', 'num_unique_words', 'num_stopwords', 'num_links', 'num_unique_domains', 'num_email_addresses', 'num_spelling_errors', 'num_urgent_keywords']\n",
      "\n",
      "Missing values:\n",
      "text                   0\n",
      "label                  0\n",
      "num_words              0\n",
      "num_unique_words       0\n",
      "num_stopwords          0\n",
      "num_links              0\n",
      "num_unique_domains     0\n",
      "num_email_addresses    0\n",
      "num_spelling_errors    0\n",
      "num_urgent_keywords    0\n",
      "dtype: int64\n",
      "\n",
      "Class distribution:\n",
      "label\n",
      "0    16306\n",
      "1     7303\n",
      "Name: count, dtype: int64\n",
      "Class balance:\n",
      "label\n",
      "0    0.690669\n",
      "1    0.309331\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Splitting data (test_size=0.2)...\n",
      "âœ… Train set: 18887 samples\n",
      "âœ… Test set: 4722 samples\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "X_train, X_test, X_text_train, X_text_test, y_train, y_test = load_and_split_data(test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "630460bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "1. LOGISTIC REGRESSION\n",
      "======================================================================\n",
      "Training Logistic Regression...\n",
      "âœ… Training complete!\n",
      "Evaluation Metrics for Logistic Regression:\n",
      "----------------------------------------------------------------------\n",
      "Accuracy:  0.6999\n",
      "Precision: 0.6196\n",
      "Recall:    0.0780\n",
      "F1-Score:  0.1386\n",
      "ROC-AUC:   0.6709\n",
      "Confusion Matrix:\n",
      "[[3191   70]\n",
      " [1347  114]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Legitimate       0.70      0.98      0.82      3261\n",
      "    Phishing       0.62      0.08      0.14      1461\n",
      "\n",
      "    accuracy                           0.70      4722\n",
      "   macro avg       0.66      0.53      0.48      4722\n",
      "weighted avg       0.68      0.70      0.61      4722\n",
      "\n",
      "\n",
      "======================================================================\n",
      "2. RANDOM FOREST\n",
      "======================================================================\n",
      "Training Random Forest...\n",
      "âœ… Training complete!\n",
      "Evaluation Metrics for Random Forest:\n",
      "----------------------------------------------------------------------\n",
      "Accuracy:  0.7956\n",
      "Precision: 0.7175\n",
      "Recall:    0.5599\n",
      "F1-Score:  0.6290\n",
      "ROC-AUC:   0.8388\n",
      "Confusion Matrix:\n",
      "[[2939  322]\n",
      " [ 643  818]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Legitimate       0.82      0.90      0.86      3261\n",
      "    Phishing       0.72      0.56      0.63      1461\n",
      "\n",
      "    accuracy                           0.80      4722\n",
      "   macro avg       0.77      0.73      0.74      4722\n",
      "weighted avg       0.79      0.80      0.79      4722\n",
      "\n",
      "Feature Importance (Random Forest):\n",
      "               feature  importance\n",
      "0            num_words    0.237595\n",
      "1     num_unique_words    0.220159\n",
      "2        num_stopwords    0.218607\n",
      "6  num_spelling_errors    0.156039\n",
      "7  num_urgent_keywords    0.069636\n",
      "5  num_email_addresses    0.042771\n",
      "3            num_links    0.030014\n",
      "4   num_unique_domains    0.025179\n",
      "\n",
      "======================================================================\n",
      "3. XGBOOST\n",
      "======================================================================\n",
      "Training XGBoost...\n",
      "âœ… Training complete!\n",
      "Evaluation Metrics for XGBoost:\n",
      "----------------------------------------------------------------------\n",
      "Accuracy:  0.7679\n",
      "Precision: 0.6887\n",
      "Recall:    0.4559\n",
      "F1-Score:  0.5486\n",
      "ROC-AUC:   0.8160\n",
      "Confusion Matrix:\n",
      "[[2960  301]\n",
      " [ 795  666]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Legitimate       0.79      0.91      0.84      3261\n",
      "    Phishing       0.69      0.46      0.55      1461\n",
      "\n",
      "    accuracy                           0.77      4722\n",
      "   macro avg       0.74      0.68      0.70      4722\n",
      "weighted avg       0.76      0.77      0.75      4722\n",
      "\n",
      "Feature Importance (XGBoost):\n",
      "               feature  importance\n",
      "4   num_unique_domains    0.282107\n",
      "5  num_email_addresses    0.156927\n",
      "7  num_urgent_keywords    0.143849\n",
      "6  num_spelling_errors    0.105778\n",
      "0            num_words    0.079387\n",
      "1     num_unique_words    0.077407\n",
      "3            num_links    0.077368\n",
      "2        num_stopwords    0.077177\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train ML models and get their results and the trained model objects\n",
    "ml_results, (lr_model, rf_model, xgb_model) = train_ml_models(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc2e0e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing datasets...\n",
      "âœ… Vocabulary size: 10000\n",
      "âœ… Train samples: 15109\n",
      "âœ… Validation samples: 3778\n",
      "âœ… Test samples: 4722\n",
      "\n",
      "======================================================================\n",
      "1. LSTM MODEL\n",
      "======================================================================\n",
      "Model architecture:\n",
      "LSTMClassifier(\n",
      "  (embedding): Embedding(10000, 128, padding_idx=0)\n",
      "  (lstm): LSTM(128, 64, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Training LSTM for 10 epochs...\n",
      "Epoch 1/10 - Train Loss: 0.3326, Train Acc: 0.8549 | Val Loss: 0.2040, Val Acc: 0.9142\n",
      "Epoch 2/10 - Train Loss: 0.1447, Train Acc: 0.9480 | Val Loss: 0.1530, Val Acc: 0.9442\n",
      "Epoch 3/10 - Train Loss: 0.0787, Train Acc: 0.9705 | Val Loss: 0.1274, Val Acc: 0.9510\n",
      "Epoch 4/10 - Train Loss: 0.0523, Train Acc: 0.9799 | Val Loss: 0.1264, Val Acc: 0.9574\n",
      "Epoch 5/10 - Train Loss: 0.0359, Train Acc: 0.9860 | Val Loss: 0.1950, Val Acc: 0.9510\n",
      "Epoch 6/10 - Train Loss: 0.0353, Train Acc: 0.9861 | Val Loss: 0.1471, Val Acc: 0.9579\n",
      "Epoch 7/10 - Train Loss: 0.0298, Train Acc: 0.9878 | Val Loss: 0.1634, Val Acc: 0.9563\n",
      "Early stopping at epoch 7\n",
      "\n",
      "âœ… Training complete!\n",
      "Evaluation Metrics for LSTM:\n",
      "----------------------------------------------------------------------\n",
      "Accuracy:  0.9555\n",
      "Precision: 0.9033\n",
      "Recall:    0.9589\n",
      "F1-Score:  0.9303\n",
      "ROC-AUC:   0.9906\n",
      "Confusion Matrix:\n",
      "[[3111  150]\n",
      " [  60 1401]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Legitimate       0.98      0.95      0.97      3261\n",
      "    Phishing       0.90      0.96      0.93      1461\n",
      "\n",
      "    accuracy                           0.96      4722\n",
      "   macro avg       0.94      0.96      0.95      4722\n",
      "weighted avg       0.96      0.96      0.96      4722\n",
      "\n",
      "\n",
      "======================================================================\n",
      "2. CNN MODEL\n",
      "======================================================================\n",
      "Model architecture:\n",
      "CNNClassifier(\n",
      "  (embedding): Embedding(10000, 128, padding_idx=0)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "    (1): Conv1d(128, 128, kernel_size=(4,), stride=(1,))\n",
      "    (2): Conv1d(128, 128, kernel_size=(5,), stride=(1,))\n",
      "  )\n",
      "  (fc1): Linear(in_features=384, out_features=64, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Training CNN for 10 epochs...\n",
      "Epoch 1/10 - Train Loss: 0.3218, Train Acc: 0.8615 | Val Loss: 0.1527, Val Acc: 0.9434\n",
      "Epoch 2/10 - Train Loss: 0.1356, Train Acc: 0.9575 | Val Loss: 0.1190, Val Acc: 0.9534\n",
      "Epoch 3/10 - Train Loss: 0.0780, Train Acc: 0.9783 | Val Loss: 0.1188, Val Acc: 0.9550\n",
      "Epoch 4/10 - Train Loss: 0.0451, Train Acc: 0.9858 | Val Loss: 0.1269, Val Acc: 0.9571\n",
      "Epoch 5/10 - Train Loss: 0.0334, Train Acc: 0.9892 | Val Loss: 0.1404, Val Acc: 0.9592\n",
      "Epoch 6/10 - Train Loss: 0.0363, Train Acc: 0.9895 | Val Loss: 0.1497, Val Acc: 0.9553\n",
      "Early stopping at epoch 6\n",
      "\n",
      "âœ… Training complete!\n",
      "\n",
      "ðŸ“Š Evaluating on test set...\n",
      "Evaluation Metrics for CNN:\n",
      "----------------------------------------------------------------------\n",
      "Accuracy:  0.9621\n",
      "Precision: 0.9109\n",
      "Recall:    0.9726\n",
      "F1-Score:  0.9407\n",
      "ROC-AUC:   0.9946\n",
      "Confusion Matrix:\n",
      "[[3122  139]\n",
      " [  40 1421]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Legitimate       0.99      0.96      0.97      3261\n",
      "    Phishing       0.91      0.97      0.94      1461\n",
      "\n",
      "    accuracy                           0.96      4722\n",
      "   macro avg       0.95      0.96      0.96      4722\n",
      "weighted avg       0.96      0.96      0.96      4722\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train DL models and get their results and the trained model objects\n",
    "dl_results, (lstm_model, cnn_model) = train_dl_models(\n",
    "    X_text_train, X_text_test, y_train, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff24befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_models(ml_results, dl_results):\n",
    "    \"\"\"Compare all models and visualize results.\"\"\"\n",
    "    \n",
    "    # Combine results\n",
    "    all_results = {**ml_results, **dl_results}\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Model': list(all_results.keys()),\n",
    "        'Accuracy': [r['accuracy'] for r in all_results.values()],\n",
    "        'Precision': [r['precision'] for r in all_results.values()],\n",
    "        'Recall': [r['recall'] for r in all_results.values()],\n",
    "        'F1-Score': [r['f1'] for r in all_results.values()],\n",
    "        'ROC-AUC': [r['roc_auc'] for r in all_results.values()]\n",
    "    })\n",
    "    \n",
    "    # Add approach column\n",
    "    comparison_df['Approach'] = comparison_df['Model'].apply(\n",
    "        lambda x: 'ML' if x in ml_results else 'DL'\n",
    "    )\n",
    "\n",
    "    print(\"Model Comparison:\")\n",
    "    print(\"=\"*70)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Save to CSV\n",
    "    comparison_df.to_csv(RESULTS_DIR / \"model_comparison.csv\", index=False)\n",
    "    print(f\"Saved comparison to: {RESULTS_DIR / 'model_comparison.csv'}\")\n",
    "    \n",
    "    # Visualizations\n",
    "    print(\"Creating visualizations...\")\n",
    "    create_visualizations(comparison_df, all_results)\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "def create_visualizations(comparison_df, all_results):\n",
    "    \"\"\"Create comparison visualizations.\"\"\"\n",
    "\n",
    "    # 1. Metrics comparison bar plot\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('ML vs DL Model Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        colors = ['skyblue' if a == 'ML' else 'lightcoral' for a in comparison_df['Approach']]\n",
    "        bars = ax.bar(comparison_df['Model'], comparison_df[metric], color=colors)\n",
    "        ax.set_title(metric, fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('Score', fontsize=12)\n",
    "        ax.set_ylim([0, 1.0])\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.3f}',\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "            \n",
    "    # Legend in the last subplot\n",
    "    axes[1, 2].axis('off')\n",
    "    axes[1, 2].legend(\n",
    "        handles=[\n",
    "            plt.Rectangle((0,0),1,1, fc='skyblue', label='ML Approach'),\n",
    "            plt.Rectangle((0,0),1,1, fc='lightcoral', label='DL Approach')\n",
    "            ],\n",
    "            loc='center',\n",
    "            fontsize=12\n",
    "        )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / 'metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\" âœ“ Saved: {RESULTS_DIR / 'metrics_comparison.png'}\")\n",
    "    plt.close()\n",
    "\n",
    "    # 2. ROC Curves comparison\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    cmap = plt.get_cmap('tab10')\n",
    "    num_models = len(all_results)\n",
    "    for idx, (model_name, results) in enumerate(all_results.items()):\n",
    "        # try/except for cases where roc cannot be computed\n",
    "        try:\n",
    "            fpr, tpr, _ = roc_curve(results['y_true'], results['y_pred_proba'])\n",
    "            auc = results['roc_auc']\n",
    "            ax.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})', linewidth=2, color=cmap(idx % 10))\n",
    "        except Exception:\n",
    "            print(f\"Could not plot ROC for {model_name}\")\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "    ax.set_title('ROC Curves - All Models', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='lower right', fontsize=10)\n",
    "    plt.savefig(RESULTS_DIR / 'roc_curves_all_models.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\" âœ“ Saved: {RESULTS_DIR / 'roc_curves_all_models.png'}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2877cf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison:\n",
      "======================================================================\n",
      "              Model  Accuracy  Precision   Recall  F1-Score  ROC-AUC Approach\n",
      "Logistic Regression  0.699915   0.619565 0.078029  0.138602 0.670872       ML\n",
      "      Random Forest  0.795637   0.717544 0.559890  0.628989 0.838759       ML\n",
      "            XGBoost  0.767895   0.688728 0.455852  0.548600 0.816012       ML\n",
      "               LSTM  0.955527   0.903288 0.958932  0.930279 0.990578       DL\n",
      "                CNN  0.962092   0.910897 0.972621  0.940748 0.994645       DL\n",
      "Saved comparison to: results/model_comparison.csv\n",
      "Creating visualizations...\n",
      " âœ“ Saved: results/metrics_comparison.png\n",
      " âœ“ Saved: results/roc_curves_all_models.png\n"
     ]
    }
   ],
   "source": [
    "# Compare all results and generate final reports and visualizations\n",
    "comparison_df = compare_all_models(ml_results, dl_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
